{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2083a2a9-2f9d-44c0-b285-1d3b04cc2253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a845a6d7baef4853a3743af5a30aec08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at sharpbai/vicuna-7b-v1.3 and are newly initialized: ['model.layers.12.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.lora_A', 'model.layers.28.self_attn.q_proj.lora_B', 'model.layers.21.self_attn.q_proj.lora_B', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.lora_A', 'model.layers.22.self_attn.q_proj.lora_A', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.lora_A', 'model.layers.9.self_attn.q_proj.lora_A', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.lora_A', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.lora_A', 'model.layers.21.self_attn.v_proj.lora_A', 'model.layers.12.self_attn.q_proj.lora_A', 'model.layers.8.self_attn.v_proj.lora_B', 'model.layers.16.self_attn.q_proj.lora_B', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.lora_A', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.lora_A', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.lora_A', 'model.layers.19.self_attn.v_proj.lora_B', 'model.layers.27.self_attn.v_proj.lora_A', 'model.layers.5.self_attn.q_proj.lora_A', 'model.layers.23.self_attn.v_proj.lora_A', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.lora_A', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.lora_B', 'model.layers.30.self_attn.v_proj.lora_A', 'model.layers.28.self_attn.q_proj.lora_A', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.lora_A', 'model.layers.31.self_attn.q_proj.lora_B', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.lora_B', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.lora_B', 'model.layers.20.self_attn.q_proj.lora_A', 'model.layers.26.self_attn.v_proj.lora_A', 'model.layers.29.self_attn.v_proj.lora_A', 'model.layers.14.self_attn.q_proj.lora_A', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.lora_B', 'model.layers.22.self_attn.q_proj.lora_B', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.lora_A', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.27.self_attn.q_proj.lora_A', 'model.layers.11.self_attn.v_proj.lora_B', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.lora_B', 'model.layers.10.self_attn.q_proj.lora_A', 'model.layers.14.self_attn.q_proj.lora_B', 'model.layers.1.self_attn.q_proj.lora_B', 'model.layers.9.self_attn.v_proj.lora_B', 'model.layers.17.self_attn.v_proj.lora_A', 'model.layers.8.self_attn.q_proj.lora_A', 'model.layers.16.self_attn.v_proj.lora_A', 'model.layers.11.self_attn.q_proj.lora_A', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.lora_B', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.lora_A', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.lora_B', 'model.layers.13.self_attn.q_proj.lora_A', 'model.layers.22.self_attn.v_proj.lora_A', 'model.layers.2.self_attn.q_proj.lora_A', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.lora_A', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.lora_B', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.lora_B', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.lora_A', 'model.layers.15.self_attn.v_proj.lora_B', 'model.layers.3.self_attn.q_proj.lora_A', 'model.layers.9.self_attn.q_proj.lora_B', 'model.layers.18.self_attn.v_proj.lora_A', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.lora_B', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.lora_B', 'model.layers.17.self_attn.q_proj.lora_B', 'model.layers.4.self_attn.v_proj.lora_B', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.lora_B', 'model.layers.8.self_attn.v_proj.lora_A', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.lora_B', 'model.layers.21.self_attn.v_proj.lora_B', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.lora_A', 'model.layers.25.self_attn.v_proj.lora_B', 'model.layers.9.self_attn.v_proj.lora_A', 'model.layers.6.self_attn.q_proj.lora_B', 'model.layers.3.self_attn.q_proj.lora_B', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.lora_B', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.lora_B', 'model.layers.12.self_attn.v_proj.lora_A', 'model.layers.1.self_attn.v_proj.lora_A', 'model.layers.31.self_attn.q_proj.lora_A', 'model.layers.26.self_attn.v_proj.lora_B', 'model.layers.28.self_attn.v_proj.lora_A', 'model.layers.0.self_attn.q_proj.lora_A', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.lora_A', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.lora_B', 'model.layers.29.self_attn.q_proj.lora_A', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.lora_B', 'model.layers.4.self_attn.q_proj.lora_A', 'model.layers.4.self_attn.v_proj.lora_A', 'model.layers.11.self_attn.v_proj.lora_A', 'model.layers.15.self_attn.v_proj.lora_A', 'model.layers.15.self_attn.q_proj.lora_A', 'model.layers.15.self_attn.q_proj.lora_B', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.lora_B', 'model.layers.13.self_attn.q_proj.lora_B', 'model.layers.29.self_attn.q_proj.lora_B', 'model.layers.30.self_attn.q_proj.lora_A', 'model.layers.23.self_attn.q_proj.lora_B', 'model.layers.26.self_attn.q_proj.lora_A', 'model.layers.16.self_attn.v_proj.lora_B', 'model.layers.14.self_attn.v_proj.lora_B', 'model.layers.8.self_attn.q_proj.lora_B', 'model.layers.30.self_attn.q_proj.lora_B', 'model.layers.0.self_attn.v_proj.lora_B', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.lora_A', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.lora_A', 'model.layers.24.self_attn.q_proj.lora_B', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.lora_A', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.lora_B', 'model.layers.10.self_attn.v_proj.lora_B', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.lora_B', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.lora_B', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.lora_B', 'model.layers.2.self_attn.q_proj.lora_B', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.lora_B', 'model.layers.12.self_attn.q_proj.lora_B', 'model.layers.7.self_attn.q_proj.lora_A', 'model.layers.19.self_attn.q_proj.lora_A', 'model.layers.27.self_attn.q_proj.lora_B', 'model.layers.6.self_attn.q_proj.lora_A', 'model.layers.4.self_attn.q_proj.lora_B', 'model.layers.20.self_attn.v_proj.lora_A', 'model.layers.1.self_attn.q_proj.lora_A', 'model.layers.22.self_attn.v_proj.lora_B', 'model.layers.26.self_attn.q_proj.lora_B', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.lora_B', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.lora_B', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.lora_A', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.lora_B', 'model.layers.24.self_attn.v_proj.lora_B']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction : if there are grammatical errors in the sentence, correct them and make the sentence again. and explain the result.\n",
      "\n",
      "input : \n",
      "\n",
      "original Sentence : \"i has cat\" \n",
      "\n",
      "Result :  \"I have a cat .\"\n",
      "\n",
      "Explanation : The original sentence is grammatically incorrect as it is missing a verb and a article. The fixed sentence adds the verb \"have\" and the article \"a\" to make it a grammatically correct sentence. The fixed sentence also suggests that the person has a cat and not just one cat. Therefore, the fixed sentence is a better sentence than the original sentence. However, it is important to note that the original sentence is still possible in some languages and it is not always possible to make every sentence grammatically correct. Therefore, it is important to consider the context and meaning of the sentence before making any corrections. \n",
      "\n",
      "Note : The original sentence is grammatically incorrect because it is missing a verb and an article. The fixed sentence adds the verb \"have\" and the article \"a\" to make it a grammatically correct sentence. The fixed sentence also suggests that the person has a cat and not just one cat. Therefore, the fixed sentence is a better\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import loralib as lora\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"grammar_fixed.csv\")\n",
    "wrong_sentences = data['wrong_sentences'].to_list()\n",
    "fixed_sentences = data['fixed_sentences'].to_list()\n",
    "results = data['result'].to_list()\n",
    "\n",
    "\n",
    "model_name = 'sharpbai/vicuna-7b-v1.3'\n",
    "model = LlamaForCausalLM.from_pretrained(model_name).to(torch.bfloat16).to(\"cuda\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.load_state_dict(torch.load('llama2.pt'), strict=False)\n",
    "\n",
    "\n",
    "\n",
    "# Generate\n",
    "# generate_ids = model.generate(inputs.input_ids.to('cuda'), max_length=30)\n",
    "for wrong_sentence, fixed_sentence, result in zip(wrong_sentences, fixed_sentences, results):\n",
    "    instruction = \"instruction : if there are grammatical errors in the sentence, correct them and make the sentence again. and explain the result.\\n\\n\"\n",
    "    prompt = instruction + 'input : \\n\\noriginal Sentence : \"' + 'i has cat'  + '\" \\n\\nResult : ' \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids.to('cuda'), max_length=256)\n",
    "    return_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print(return_text)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54bf3d5-d306-491c-bf4b-63bd44c2b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction : if there are grammatical errors in the sentence, correct them and make the sentence again. and explain the result.\n",
      "\n",
      "input : \n",
      "\n",
      "original Sentence : \"New and new technology has been introduced to the society .\" \n",
      "\n",
      "Result :  \"New technology has been introduced to society .\"\n",
      "\n",
      "Explanation : There is nothing grammatically wrong with the original sentence, but the fixed sentence is more concise and eliminates redundancy. The word \"new\" is repeated unnecessarily, and the phrase \"to the society\" is not necessary for clarity or meaning. The fixed sentence is a more effective and efficient way of conveying the same idea. However, it is worth noting that the original sentence could be interpreted as suggesting that new technology is exclusively for the benefit of society, which may not be entirely accurate. The fixed sentence does not convey this implication.\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "# generate_ids = model.generate(inputs.input_ids.to('cuda'), max_length=30)\n",
    "for wrong_sentence, fixed_sentence, result in zip(wrong_sentences, fixed_sentences, results):\n",
    "    instruction = \"instruction : if there are grammatical errors in the sentence, correct them and make the sentence again. and explain the result.\\n\\n\"\n",
    "    prompt = instruction + 'input : \\n\\noriginal Sentence : \"' + wrong_sentence  + '\" \\n\\nResult : ' \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids.to('cuda'), max_length=256)\n",
    "    return_text = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print(return_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953c67b-ec87-4bd0-af42-4f8170c4ee39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
